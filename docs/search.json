[
  {
    "objectID": "demo-mdp.html",
    "href": "demo-mdp.html",
    "title": "MDP planning and inverse planning",
    "section": "",
    "text": "In this notebook we will set up a simple grid-world, plan routes to goals, and infer goals given actions.\n\nfrom functools import cache\nimport jax\nimport jax.numpy as np\nimport matplotlib.pyplot as plt\nfrom memo import memo\n\nH = 21\nW = 21\nS = np.arange(H * W)  # state space\nG = np.array([0, H * W - 1])  # possible goals: NW and SE corners\n\nA = np.array([0, 1, 2, 3])  # action space: left, right, up, down\ncoord_actions = np.array([[-1, 0], [+1, 0], [0, -1], [0, +1]])\n\nmaze_raw = np.array(1 - plt.imread('src/assets/img/logo-maze.png'), dtype=int)\nmaze = maze_raw.reshape(-1)\nassert maze_raw.size == H * W\n\n# # Alternatively...\n# maze = np.zeros(H * W)  # blank maze\n\n# transition function: P(s_ | s, a)\n@jax.jit\ndef Tr(s, a, s_):\n    x, y = s % W, s // W\n    next_coords = np.array([x, y]) + coord_actions[a]\n    next_state = (\n        + 1 * np.clip(next_coords[0], 0, W - 1)\n        + W * np.clip(next_coords[1], 0, H - 1)\n    )\n    return (\n        + 1.0 * ((maze[next_state] == 0) & (next_state == s_))  # next state free, can move there\n        + 1.0 * ((maze[next_state] == 1) & (s == s_)) # next state occupied, stay where you are\n    )\n\n# reward function\n@jax.jit\ndef R(s, a, g):\n    return 1.0 * (s == g) - 0.1\n\n@jax.jit\ndef is_terminating(s, g):\n    return s == g\n\n# discount factor\n@jax.jit\ndef gamma():\n    return 1.0\n\nprint('loaded!')\n\nWe can plan via Q-value iteration and inverse-plan by inferring \\(P(g \\mid s, a)\\) where \\(P(a \\mid s, g)\\) is given by a softmax over Q-value with \\(\\beta=2\\).\n\n@cache\n@memo\ndef Q[s: S, a: A, g: G](t):\n    alice: knows(s, a, g)\n    alice: given(s_ in S, wpp=Tr(s, a, s_))\n    alice: chooses(a_ in A, to_maximize=0.0 if t &lt; 0 else Q[s_, a_, g](t - 1))\n    return E[\n        R(s, a, g) + (0.0 if t &lt; 0 else\n                      0.0 if is_terminating(s, g) else\n                      gamma() * Q[alice.s_, alice.a_, g](t - 1))\n    ]\n\n@memo\ndef invplan[s: S, a: A, g: G](t):\n    observer: knows(a, s, g)\n    observer: thinks[\n        alice: chooses(g in G, wpp=1),\n        alice: knows(s),\n        alice: chooses(a in A, wpp=exp(2 * Q[s, a, g](t))),\n    ]\n    observer: observes [alice.a] is a\n    return observer[E[alice.g == g]]\n\nQ(0)  # pre-compile Q\nprint(\"starting...\")\nip = invplan(100)\nv = Q(100).max(axis=1)\nprint(\"done!\")\n\nThis is already pretty fast, though it is even faster on a GPU.\nFinally, let’s make the plots shown in the paper.\n\nplt.figure(figsize=(3, 3))\n\nplt.subplot(2, 2, 1)\nplt.imshow((v[:, 0].reshape(H, W)))\nplt.imshow(1 - maze_raw, cmap='gray', alpha=1. * maze_raw)\nplt.plot([0], [0], 'r*')\nplt.plot([20], [20], 'b*')\nplt.xticks([]); plt.yticks([])\nplt.title('(a)')\n\nplt.subplot(2, 2, 2)\nplt.imshow((v[:, 1].reshape(H, W)))\nplt.imshow(1 - maze_raw, cmap='gray', alpha=1. * maze_raw)\nplt.xticks([]); plt.yticks([])\nplt.plot([0], [0], 'r*')\nplt.plot([20], [20], 'b*')\nplt.title('(b)')\n\ndip = ip[:, :, 0] - ip[:, :, 1]\nplt.subplot(2, 2, 3)\nplt.imshow(dip[:, 0].reshape(H, W), cmap='bwr', vmin=-1, vmax=+1)\nplt.imshow(1 - maze_raw, cmap='gray', alpha=1. * maze_raw)\nplt.xticks([]); plt.yticks([])\nplt.plot([0], [0], 'r*')\nplt.plot([20], [20], 'b*')\nplt.title('(c)')\n\nplt.subplot(2, 2, 4)\nplt.imshow(dip[:, 3].reshape(H, W), cmap='bwr', vmin=-1, vmax=+1)\nplt.imshow(1 - maze_raw, cmap='gray', alpha=1. * maze_raw)\nplt.xticks([]); plt.yticks([])\nplt.plot([0], [0], 'r*')\nplt.plot([20], [20], 'b*')\nplt.title('(d)')\n\nplt.tight_layout()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "memo",
    "section": "",
    "text": "from memo import memo\nimport jax\nimport jax.numpy as np\nfrom enum import IntEnum\n\nprint(jax.__version__)\n\n\nSuggested demos\n\nScalar Implicature\nQuestion-asking via expected information gain\nPlanning and inverse-planning in an MDP\n\n\n\n\nmemo resources\n\nmemo GitHub Repo\nThe memo handbook\nGitHub Discussions - To ask questions about memo, and to get help from other memo users.\nMailing list - For updates on memo’s development."
  },
  {
    "objectID": "demo-scalar-implicature.html",
    "href": "demo-scalar-implicature.html",
    "title": "Scalar Implicature",
    "section": "",
    "text": "from memo import memo\nimport jax\nimport jax.numpy as np\nfrom enum import IntEnum\n\n## Scalar implicature\n\nNN = 10_000\n\nN = np.arange(NN + 1)  # number of nice people\nclass U(IntEnum):\n    NONE = 0\n    SOME = 1\n    ALL = 2\n\n@jax.jit\ndef meaning(n, u):  # (none)  (some)  (all)\n    return np.array([ n == 0, n &gt; 0, n == NN ])[u]\n\n@memo\ndef scalar[n: N, u: U]():\n    listener: thinks[\n        speaker: chooses(n in N, wpp=1),\n        speaker: chooses(u in U, wpp=imagine[\n            listener: knows(u),\n            listener: chooses(n in N, wpp=meaning(n, u)),\n            Pr[listener.n == n]\n        ])\n    ]\n    listener: observes [speaker.u] is u\n    listener: chooses(n in N, wpp=E[speaker.n == n])\n    return Pr[listener.n == n]\n\nscalar()  # warm up JIT\n\nimport time\nt_s = time.time()\nscalar()\nprint(time.time() - t_s)"
  },
  {
    "objectID": "demo-eig.html",
    "href": "demo-eig.html",
    "title": "Question-asking based on Expected Information Gain (EIG)",
    "section": "",
    "text": "Inspired by: Rothe, A., Lake, B. M., & Gureckis, T. M. (2018). Do people ask good questions?. Computational Brain & Behavior, 1, 69-89.\nBob rolls a red die and a blue die. Alice gets to ask one yes-no question about the sum. What is the most informative question she could ask, in order to learn the most about the two die rolls? For example, is it better to ask if the sum is a perfect square, or if the sum is prime?\nWe’ll compute the EIG of various questions…\n\nfrom memo import memo\nimport jax\nimport jax.numpy as np\n\nis_prime  = np.array([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0])\nis_square = np.array([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0])\nis_pow_2  = np.array([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\nQs = [\n    lambda n: n == 7,\n    lambda n: n == 12,\n    lambda n: n &gt; 10,\n    lambda n: n &gt; 8,\n    lambda n: n &gt; 6,\n    lambda n: n &gt; 5,\n    lambda n: n % 2 == 0,  # even??\n    lambda n: n % 2 == 1,  # odd??\n    lambda n: n % 3 == 0,\n    lambda n: n % 4 == 0,\n    lambda n: n % 5 == 0,\n    lambda n: is_prime[n],\n    lambda n: is_square[n],\n    lambda n: is_pow_2[n],\n]\n\nN = np.arange(1, 6 + 1)  # single die's outcomes\nQ = np.arange(len(Qs))   # questions\nA = np.array([0, 1])     # answers (yes/no)\n\n@jax.jit\ndef respond(q, a, n):\n    return np.array([q_(n) for q_ in Qs])[q] == a\n\n@memo\ndef eig[q: Q]():\n    alice: knows(q)\n    alice: thinks[\n        # bob rolls dice...\n        bob: chooses(n_red in N, wpp=1),\n        bob: chooses(n_blu in N, wpp=1),\n\n        # bob answers question...\n        bob: knows(q),\n        bob: chooses(a in A, wpp=respond(q, a, n_red + n_blu))\n    ]\n    alice: snapshots_self_as(future_self)\n\n    return alice[ imagine[\n        # if I were to get the answer...\n        future_self: observes [bob.a] is bob.a,\n        # EIG = entropy minus conditional entropy\n        H[bob.n_red, bob.n_blu] - E[future_self[ H[bob.n_red, bob.n_blu] ]]\n    ] ]\n\nz = eig()\n%timeit -r 10 -n 100 eig().block_until_ready()\n\n## print questions and EIGs in sorted order\nprint('EIG     Question')\nprint('---     ---')\nimport inspect\nq_names = [inspect.getsource(q_).strip()[10:-1] for q_ in Qs]\nfor eig_, q_ in reversed(sorted(list(zip(z, q_names)))):\n    print(f'{eig_:0.5f}', q_)"
  },
  {
    "objectID": "rsa.html",
    "href": "rsa.html",
    "title": "RSA",
    "section": "",
    "text": "from memo import memo\nimport jax\nimport jax.numpy as np\nfrom enum import IntEnum\n\nprint(jax.__version__)\n\nclass U(IntEnum):  # utterance space\n    GREEN  = 0\n    PINK   = 1\n    SQUARE = 2\n    ROUND  = 3\n\nclass R(IntEnum):  # referent space\n    GREEN_SQUARE = 0\n    GREEN_CIRCLE = 1\n    PINK_CIRCLE  = 2\n\n@jax.jit\ndef denotes(u, r):\n    return np.array([\n    #    green square\n    #    |  green circle\n    #    |  |  pink circle\n    #    |  |  |\n        [1, 1, 0],  # \"green\"\n        [0, 0, 1],  # \"pink\"\n        [1, 0, 0],  # \"square\"\n        [0, 1, 1]   # \"round\"\n    ])[u, r]\n\n\n@memo\ndef L[u: U, r: R](beta, t):\n    listener: thinks[\n        speaker: given(r in R, wpp=1),\n        speaker: chooses(u in U, wpp=\n            denotes(u, r) * (1 if t == 0 else exp(beta * L[u, r](beta, t - 1))))\n    ]\n    listener: observes [speaker.u] is u\n    listener: chooses(r in R, wpp=Pr[speaker.r == r])\n    return Pr[listener.r == r]\n\nbeta = 1.\nprint(L(beta, 0))\nprint(L(beta, 1))\n\n\n## Fitting the model to data...\nY = np.array([65, 115, 0]) / 180  # data from Qing & Franke 2015\n@jax.jit\ndef loss(beta):\n    return np.mean((L(beta, 1)[0] - Y) ** 2)\n\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(5, 4))\n\n## Fitting by gradient descent!\nvg = jax.value_and_grad(loss)\nlosses = []\nbeta = 0.\nfor _ in range(26):\n    l, dbeta = vg(beta)\n    losses.append(l)\n    beta = beta - dbeta * 12.\nplt.plot(np.arange(len(losses)), losses)\nplt.ylabel('MSE (%)')\nplt.xlabel('Step #')\nplt.yticks([0, 0.02], [0, 2])\nplt.title('Gradient descent')\n\nplt.tight_layout()"
  },
  {
    "objectID": "game23.html#memo-models-are-decorated-functions",
    "href": "game23.html#memo-models-are-decorated-functions",
    "title": "2/3rds Game",
    "section": "memo models are decorated functions",
    "text": "memo models are decorated functions\n\nfrom memo import memo\n\nX = [1, 2, 3]\nY = range(10)\n\n@memo\ndef my_model[x: X, y: Y](a, b=2, c=None):\n    return x + y\n\nmy_model(1, b=2, c=3)\n\n\nYou can convert the output array into an pandas DataFrame or an xarray for easy indexing:\n\nres = my_model(1, b=2, c=3, \n                        return_aux=True, \n                        return_pandas=True, \n                        return_xarray=True)\ndata = res.data\ndf = res.aux.pandas\nxa = res.aux.xarray\nprint(f\"===JAX array===\"); print(data)\nprint(f\"\\n===pandas===\"); print(df.head())\nprint(f\"\\n===xarray===\"); print(xa)\n\n\nchooses()\n\nimport jax.numpy as np\n\nZ = np.arange(100) + 1\n\n@memo\ndef my_model[z: Z]():\n    kartik: chooses(z in Z, wpp=1)\n    return E[kartik.z == z]\n\nmy_model()\n\n\n\nexpressions\n\nimport jax.numpy as np\n\nZ = np.arange(100) + 1\n\n@memo\ndef my_model[z: Z]():\n    kartik: chooses(z in Z, wpp=1)\n    return Pr[exp(kartik.z) &gt; 5]\n\nmy_model()\n\n\n\n@memo\ndef my_model[z: Z]():\n    kartik: chooses(z in Z, wpp=3 if z % 2 == 0 else 1)\n    return E[kartik.z == z]\n\nmy_model()\n\n\n\nfrom matplotlib import pyplot as plt\nimport jax\nfrom jax.scipy.stats.norm import pdf as normpdf\n\ngaussianpdf = jax.jit(normpdf)\n\n@memo\ndef my_model[z: Z]():\n    kartik: chooses(z in Z, wpp=gaussianpdf(z, 20, 5))\n    return E[kartik.z == z]\n\nplt.plot(Z, my_model())\n\n\n\nthinks[]\n\n@memo\ndef my_model[z: Z]():\n    kartik: thinks[\n        maxkw: chooses(z in Z, wpp=3 if z % 2 == 0 else 1)\n    ]\n    return kartik[ E[maxkw.z == 1]]\n\nmy_model()"
  }
]